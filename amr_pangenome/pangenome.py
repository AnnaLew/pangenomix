#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Created on Thu Mar 12 10:18:29 2020

@author: jhyun95

Pan-genome construction tools including consolidating redundant sequences,
gene sequence cluster identification by CD-Hit, and constructing gene/allele tables.
Refer to build_cds_pangenome() and build_upstream_pangenome().
"""

import os
import subprocess as sp
import hashlib 
import collections

import pandas as pd
import numpy as np
import scipy.sparse

DNA_COMPLEMENT = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 
              'W': 'W', 'S': 'S', 'R': 'Y', 'Y': 'R', 
              'M': 'K', 'K': 'M', 'N': 'N'}
for bp in DNA_COMPLEMENT.keys():
    DNA_COMPLEMENT[bp.lower()] = DNA_COMPLEMENT[bp].lower()

def build_cds_pangenome(genome_faa_paths, output_dir, name='Test', 
                        cdhit_args={'-n':5, '-c':0.8}, fastasort_path=None):
    ''' 
    Constructs a pan-genome based on protein sequences with the following steps:
    1) Merge FAA files for genomes of interest into a non-redundant list
    2) Cluster CDS by sequence into putative genes using CD-Hit
    3) Rename non-redundant CDS as <name>_C#A#, referring to cluster and allele number
    4) Compile allele/gene membership into binary allele x genome and gene x genome tables
    
    Generates eight files within output_dir:
    1) <name>_strain_by_allele.pickle.gz, binary allele x genome table with SparseArray structure
    2) <name>_strain_by_gene.pickle.gz, binary gene x genome table with SparseArray structure
    1) <name>_strain_by_allele.csv.gz, binary allele x genome table as flat file
    2) <name>_strain_by_gene.csv.gz, binary gene x genome table as flat file
    3) <name>_nr.faa, all non-redundant CDSs observed, with headers <name>_C#A#
    4) <name>_nr.faa.cdhit.clstr, CD-Hit output file from clustering
    5) <name>_allele_names.tsv, mapping between <name>_C#A# to original CDS headers
    6) <name>_redundant_headers.tsv, lists of headers sharing the same CDS, with the
        representative header relevant to #5 listed first for each group.
    7) <name>_missing_headers.txt, lists headers for original entries missing sequences
    
    Parameters
    ----------
    genome_faa_paths : list 
        FAA files containing CDSs for genomes of interest. Genome 
        names are inferred from these FAA file paths.
    output_dir : str
        Path to directory to generate outputs and intermediates.
    name : str
        Header to prepend to all output files and allele names (default 'Test')
    cdhit_args : dict
        Alignment arguments to pass CD-Hit, other than -i, -o, and -d
        (default {'-n':5, '-c':0.8})
    fastasort_path : str
        Path to Exonerate's fastasort binary, optionally for sorting
        final FAA files (default None)
        
    Returns 
    -------
    df_alleles : pd.DataFrame
        Binary allele x genome table
    df_genes : pd.DataFrame
        Binary gene x genome table
    '''
    
    ''' Merge FAAs into one file with non-redundant sequences '''
    print 'Identifying non-redundant CDS sequences...'
    output_nr_faa = output_dir + '/' + name + '_nr.faa' # final non-redundant FAA files
    output_shared_headers = output_dir + '/' + name + '_redundant_headers.tsv' # records headers that have the same sequence
    output_missing_headers = output_dir + '/' + name + '_missing_headers.txt' # records headers without any seqeunce
    output_nr_faa = output_nr_faa.replace('//','/')
    output_shared_headers = output_shared_headers.replace('//','/')
    output_missing_headers = output_missing_headers.replace('//','/')
    non_redundant_seq_hashes, missing_headers = consolidate_cds(genome_faa_paths, output_nr_faa, 
                                                                output_shared_headers, output_missing_headers)
    # maps sequence hash to headers of that sequence, in order observed
    
    ''' Apply CD-Hit to non-redundant CDS sequences '''
    output_nr_faa_copy = output_nr_faa + '.cdhit' # temporary FAA copy generated by CD-Hit
    output_nr_clstr = output_nr_faa + '.cdhit.clstr' # cluster file generated by CD-Hit
    cluster_cds(output_nr_faa, output_nr_faa_copy, cdhit_args)
    os.remove(output_nr_faa_copy) # delete CD-hit copied sequences
    
    ''' Extract genes and alleles, rename unique sequences as <name>_C#A# '''
    output_allele_names = output_dir + '/' + name + '_allele_names.tsv' # allele names vs non-redundant headers
    output_allele_names = output_allele_names.replace('//','/')
    header_to_allele = rename_genes_and_alleles(output_nr_clstr, output_nr_faa, output_nr_faa, 
                                                output_allele_names, name=name,
                                                shared_headers_file=output_shared_headers,
                                                fastasort_path=fastasort_path)
    # maps original headers to short names <name>_C#A#
    
    ''' Process gene/allele membership into binary tables '''    
    df_alleles, df_genes = build_genetic_feature_tables(output_nr_clstr, genome_faa_paths, 
                              name, header_to_allele=header_to_allele)
    
    ''' Save tables as PICKLE.GZ (preserve SparseArrays) and CSV.GZ (backup flat file) '''
    output_allele_table = output_dir + '/' + name + '_strain_by_allele'
    output_gene_table = output_dir + '/' + name + '_strain_by_gene'
    output_allele_table = output_allele_table.replace('//','/')
    output_gene_table = output_gene_table.replace('//','/')
    output_allele_csv = output_allele_table + '.csv.gz'
    output_gene_csv = output_gene_table + '.csv.gz'
    output_allele_pickle = output_allele_table + '.pickle.gz'
    output_gene_pickle = output_gene_table + '.pickle.gz'
    print 'Saving', output_allele_pickle, '...'
    df_alleles.to_pickle(output_allele_pickle)
    print 'Saving', output_allele_csv, '...'
    df_alleles.to_csv(output_allele_csv)
    print 'Saving', output_gene_pickle, '...'
    df_genes.to_pickle(output_gene_pickle)
    print 'Saving', output_gene_csv, '...'
    df_alleles.to_csv(output_gene_csv)
    
    return df_alleles, df_genes
    

def consolidate_cds(genome_faa_paths, nr_faa_out, shared_headers_out, missing_headers_out=None):
    ''' 
    Combines CDS protein sequences for many genomes into a single file while
    without duplicate sequences to be clustered using cluster_cds (CD-Hit wrapper),
    Tracks headers that share the same sequence, and optionally headers without sequences.
    
    Parameters
    ----------
    genome_faa_paths : list
        Paths to genome FAA files to combine
    nr_faa_out : str
        Output path for combined non-redundant FAA file
    shared_headers_out : str
        Output path for shared headers TSV file
    missing_headers_out : str
        Output path for headers without sequences TXT file (default None)

    Returns
    -------
    non_redundant_seq_hashes : dict
        Maps non-redundant sequence hashes to a list of headers, in order observed
    missing_headers : list
        List of headers without any associated sequence
    '''
    non_redundant_seq_hashes = {} # maps sequence hash to headers of that sequence, in order observed
    encounter_order = [] # stores sequence hashes in order encountered
    missing_headers = [] # stores headers without sequences
    
    def process_header_and_seq(header, seq_blocks, output_file):
        ''' Processes a header/sequence pair against the running list of non-redundant sequences '''
        seq = ''.join(seq_blocks)
        if len(header) > 0 and len(seq) > 0: # valid header-sequence record
            seqhash = __hash_sequence__(seq)
            if seqhash in non_redundant_seq_hashes: # record repeated appearances of sequence
                non_redundant_seq_hashes[seqhash].append(header)
            else: # first encounter of a sequence, record to non-redundant FAA file
                encounter_order.append(seqhash)
                non_redundant_seq_hashes[seqhash] = [header]
                output_file.write('>' + header + '\n')
                output_file.write('\n'.join(seq_blocks) + '\n')
        elif len(header) > 0 and len(seq) == 0: # header without sequence
            missing_headers.append(header)
    
    ''' Scan for redundant CDS for all FAA files, build non-redundant file '''
    with open(nr_faa_out, 'w+') as f_nr_out:
        for faa_path in genome_faa_paths:
            with open(faa_path, 'r') as f:
                header = ''; seq_blocks = []
                for line in f:
                    if line[0] == '>': # header encountered
                        process_header_and_seq(header, seq_blocks, f_nr_out)
                        header = __get_header_from_fasta_line__(line)
                        seq_blocks = []
                    else: # sequence line encountered
                        seq_blocks.append(line.strip())
                process_header_and_seq(header, seq_blocks, f_nr_out) # process last record
                
    ''' Save shared and missing headers to file '''
    with open(shared_headers_out, 'w+') as f_header_out:
        for seqhash in encounter_order:
            headers = non_redundant_seq_hashes[seqhash]
            if len(headers) > 1:
                f_header_out.write('\t'.join(headers) + '\n')
    if missing_headers_out:
        print 'Headers without sequences:', len(missing_headers)
        with open(missing_headers_out, 'w+') as f_header_out:
            for header in missing_headers:
                f_header_out.write(header + '\n')
                
    return non_redundant_seq_hashes, missing_headers
        
                
def cluster_cds(faa_file, cdhit_out, cdhit_args={'-n':5, '-c':0.8}):
    '''
    Runs CD-Hit on a FAA file (i.e. one generated by consolidate_cds).
    Requires cd-hit to be available in PATH.
    
    Parameters
    ----------
    faa_file : str
        Path to FAA file to be clustered, i.e. from consolidate_cds
    cdhit_out : str
        Path to be provided to CD-Hit output argument
    cdhit_args : dict
        Dictionary of alignment arguments to be provided to CD-Hit, other than
        -i, -o, and -d. (default {'-n':f, '-c':0.8})
    ''' 
    args = ['cd-hit', '-i', faa_file, '-o', cdhit_out, '-d', '0']
    for arg in cdhit_args:
        args += [arg, str(cdhit_args[arg])]
    print 'Running:', args
    for line in __stream_stdout__(' '.join(args)):
        print line

def rename_genes_and_alleles(clstr_file, nr_faa_file, nr_faa_out, feature_names_out, name='Test',
                             shared_headers_file=None, fastasort_path=None):
    '''
    Processes a CD-Hit CLSTR file (clstr_file) to rename headers in the orignal
    FAA (nr_faa_file) as <name>_C#A# based on cluster membership and stores header-name
    mappings as a TSV (feature_names_out).
    
    Can optionally sort final FAA file if fastasort_path is specified, from Exonerate
    https://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate
    
    Parameters
    ----------
    clstr_file : str
        Path to CLSTR file generated by CD-Hit
    nr_faa_file : str
        Path to FAA file corresponding to clstr_file
    nr_faa_out : str
        Output path for renamed FAA, will overwrite if equal to nr_faa
    feature_names_out : str
        Output path for header-allele name mapping TSV file
    name : str
        Header to append output files and allele names (default 'Test')
    shared_headers_file : str
        Path to shared headers. If provided, will expand the header-allele
        mapping to include headers that map to the same sequence/allele (default None)
    fastasort_path : str
        Path to Exonerate fastasort, used to optionally sort nr_faa (default None)
        
    Returns
    -------
    header_to_allele : dict
        Maps original headers to new allele names
    '''
    
    ''' Optionally, load up shared headers '''
    shared_headers = {} # maps representative header to synonym headers
    if shared_headers_file:
        with open(shared_headers_file, 'r') as f_share:
            for line in f_share:
                headers = line.strip().split('\t')
                representative_header = headers[0]
                synonym_headers = headers[1:]
                shared_headers[representative_header] = synonym_headers
    
    ''' Read through CLSTR file to map original headers to C#A# names '''
    header_to_allele = {} # maps headers to allele name (name_C#A#)
    max_cluster = 0 
    with open(feature_names_out, 'w+') as f_naming:
        with open(clstr_file, 'r') as f_clstr:
            for line in f_clstr:
                if line[0] == '>': # starting new gene cluster
                    cluster_num = line.split()[-1].strip() # cluster number as string
                    max_cluster = cluster_num
                else: # adding allele to cluster
                    data = line.split()
                    allele_num = data[0] # allele number as string
                    allele_header = data[2][1:-3] # old allele header
                    allele_name = name + '_C' + cluster_num + 'A' + allele_num # new short header
                    header_to_allele[allele_header] = allele_name
                    mapped_headers = [allele_header]
                    if allele_header in shared_headers: # if synonym headers are available
                        for synonym_header in shared_headers[allele_header]:
                            header_to_allele[synonym_header] = allele_name
                        mapped_headers += shared_headers[allele_header]
                    f_naming.write(allele_name + '\t' + ('\t'.join(mapped_headers)).strip() + '\n')
                    
    ''' Create the FAA file with renamed features '''
    with open(nr_faa_file, 'r') as f_faa_old:
        with open(nr_faa_out + '.tmp', 'w+') as f_faa_new:
            ''' Iterate through alleles in cluster/allele order '''
            for line in f_faa_old:
                if line[0] == '>': # writing updated header line
                    allele_header = line[1:].strip()
                    if allele_header in header_to_allele:
                        allele_name = header_to_allele[allele_header]
                    else:
                        print 'MISSING:', allele_header
                    f_faa_new.write('>' + allele_name + '\n')
                else: # writing sequence line
                    f_faa_new.write(line)
    
    ''' Move FAA file to desired output path '''
    if nr_faa_out == nr_faa_file: # if overwriting, remove old faa file
        os.remove(nr_faa_file) 
    os.rename(nr_faa_out + '.tmp', nr_faa_out)
    
    ''' If available, use exonerate.fastasort to sort entries in fasta file '''
    if fastasort_path:
        print 'Sorting sequences by header...'
        args = ['./' + fastasort_path, nr_faa_out]
        with open(nr_faa_out + '.tmp', 'w+') as f_sort:
            sp.call(args, stdout=f_sort)
        os.rename(nr_faa_out + '.tmp', nr_faa_out)
    return header_to_allele
    

def build_genetic_feature_tables(clstr_file, genome_faa_paths, name='Test', 
                                 shared_header_file=None, header_to_allele=None):
    '''
    Builds two binary tables based on the presence/absence of genetic features, 
    allele x genome (allele_table_out) and gene x genome (gene_table_out). 
    Uses a CD-Hit CLSTR file, the corresponding original FAA files, and 
    shared header mappings.
    
    Parameters
    ----------
    clstr_file : str
        Path to CD-Hit CLSTR file used to build header-allele mappings
    genome_faa_paths : list
        Paths to genome FAA files originally combined and clustered (see consolidate_cds)
    name : str
        Name to attach to features and files (default 'Test')
    shared_header_file : str
        Path to shared header TSV file, if synonym headers are not mapped
        in header_to_allele or header_to_allele is not provided (default None)
    header_to_allele : dict
        Pre-calculated header-allele mappings corresponding to clstr_file,
        if available from rename_genes_and_alleles (default None)

    Returns 
    -------
    df_alleles : pd.DataFrame
        Binary allele x genome table
    df_genes : pd.DataFrame
        Binary gene x genome table
    '''
    
    ''' Load header-allele mappings '''
    print 'Loadings header-allele mappings...'
    header_to_allele = load_header_to_allele(clstr_file, shared_header_file, header_to_allele)
                    
    ''' Initialize gene and allele tables '''
    faa_to_genome = lambda x: x.split('/')[-1][:-4]
    genome_order = sorted([faa_to_genome(x) for x in genome_faa_paths]) # for genome names, trim .faa from filenames
    print 'Sorting alleles...'
    allele_order = sorted(list(set(header_to_allele.values())))
    
    print 'Sorting genes...'
    gene_order = []; last_gene = None
    for allele in allele_order:
        gene = __get_gene_from_allele__(allele)
        if gene != last_gene:
            gene_order.append(gene)
            last_gene = gene
    print 'Genomes:', len(genome_order)
    print 'Alleles:', len(allele_order)
    print 'Genes:', len(gene_order)
        
    ''' To use sparse matrix, map genomes, alleles, and genes to positions '''
    allele_indices = {allele_order[i]:i for i in range(len(allele_order))}
    gene_indices = {gene_order[i]:i for i in range(len(gene_order))}   
    allele_arrays = {} # maps genome:allele vectors as SparseArrays
    gene_arrays = {} # maps genome:gene vector as SparseArrays

    ''' Scan original genome file for allele and gene membership '''
    for i, genome_faa in enumerate(sorted(genome_faa_paths)):
        genome = faa_to_genome(genome_faa)
        genome_i = genome_order.index(genome)
        allele_arrays[genome] = np.zeros(shape=len(allele_order), dtype='int64')
        gene_arrays[genome] = np.zeros(shape=len(gene_order), dtype='int64')
        with open(genome_faa, 'r') as f_faa:
            header = ''; seq = '' # track the sequence to skip over empty sequences
            for line in f_faa.readlines():
                ''' Load all alleles and genes per genome '''
                if line[0] == '>': # new header line encountered
                    if len(seq) > 0:
                        if header in header_to_allele:
                            allele_name = header_to_allele[header]
                            allele_i = allele_indices[allele_name]
                            allele_arrays[genome][allele_i] = 1
                            gene = __get_gene_from_allele__(allele_name)
                            gene_i = gene_indices[gene]
                            gene_arrays[genome][gene_i] = 1
                        else:
                            print 'MISSING:', allele_header
                    header = __get_header_from_fasta_line__(line)
                    seq = '' # reset sequence
                else: # sequence line encountered
                    seq += line.strip()
            if len(seq) > 0: # process last record
                if header in header_to_allele:
                    allele_name = header_to_allele[header]
                    allele_i = allele_indices[allele_name]
                    allele_arrays[genome][allele_i] = 1
                    gene = __get_gene_from_allele__(allele_name)
                    gene_i = gene_indices[gene]
                    gene_arrays[genome][gene_i] = 1
                else:
                    print 'MISSING:', header
                
        allele_arrays[genome] = pd.SparseArray(allele_arrays[genome])
        gene_arrays[genome] = pd.SparseArray(gene_arrays[genome])
        allele_arrays[genome].fill_value = np.nan
        gene_arrays[genome].fill_value = np.nan
        print 'Updating genome', i+1, ':', genome, 
        print '\tAlleles:', allele_arrays[genome].sum(), '\tGenes:', gene_arrays[genome].sum()
        
    ''' Construct DataFrame '''
    print 'Building DataFrame...'
    df_alleles = pd.DataFrame(data=allele_arrays, index=allele_order)
    df_genes = pd.DataFrame(data=gene_arrays, index=gene_order)
    return df_alleles, df_genes


def load_header_to_allele(clstr_file=None, shared_header_file=None, header_to_allele=None, name='Test'):
    '''
    Loads a mapping from original fasta headers to allele names format name_C#A#.
    
    Parameters
    ----------
    clstr_file : str
        Path to CD-Hit CLSTR file used to build header-allele mappings,
        only needed if header_to_allele is None (default None)
    shared_header_file : str
        Path to shared header TSV file, if synonym headers are not mapped
        in header_to_allele or header_to_allele is not provided (default None)
    header_to_allele : dict
        Pre-calculated header-allele mappings corresponding to clstr_file,
        if available from rename_genes_and_alleles (default None)
    name : str
        Name to attach to features and files (default 'Test')
        
    Returns
    -------
    full_header_to_allele : dict
        Full header-allele mappings combining contents of both header_to_allele 
        (copied or built from clstr_file) and shared_header_file.
    '''
    
    ''' Load header to allele mapping from CLSTR, if not provided '''
    if header_to_allele is None:
        full_header_to_allele = {} # maps representative header to allele name (name_C#A#)
        with open(clstr_file, 'r') as f_clstr:
            for line in f_clstr:
                if line[0] == '>': # starting new gene cluster
                    cluster_num = line.split()[-1].strip() # cluster number as string
                    max_cluster = cluster_num
                else: # adding allele to cluster
                    data = line.split()
                    allele_num = data[0] # allele number as string
                    allele_header = data[2][1:-3] # old allele header
                    allele_name = name + '_C' + cluster_num + 'A' + allele_num # new short header
                    full_header_to_allele[allele_header] = allele_name
    elif type(header_to_allele) == dict:
        full_header_to_allele = header_to_allele.copy()
    
    ''' Load headers that share the same sequence '''
    if shared_header_file:
        with open(shared_header_file, 'r') as f_header:
            for line in f_header:
                headers = [x.strip() for x in line.split('\t')]
                if len(headers) > 1:
                    repr_header = headers[0]
                    repr_allele = full_header_to_allele[repr_header]
                    for alt_header in headers[1:]:
                        full_header_to_allele[alt_header] = repr_allele
    return full_header_to_allele


def build_upstream_pangenome(genome_data, allele_names, output_dir, limits=(-50,3), name='Test', 
                             include_fragments=False, fastasort_path=None):
    '''
    Extracts nucleotides upstream of coding sequences for multiple genomes, 
    create <genome>_upstream.fna files in the same directory for each genome.
    Then, classifies/names them relative to gene clusters identified by coding sequence,  
    i.e. after build_cds_pangenome(). See build_proximal_pangenome() for parameters.
    '''
    return build_proximal_pangenome(genome_data, allele_names, output_dir, limits, side='up',
                name=name, include_fragments=include_fragments, fastasort_path=fastasort_path)
    

def build_downstream_pangenome(genome_data, allele_names, output_dir, limits=(-3,50), name='Test', 
                               include_fragments=False, fastasort_path=None):
    '''
    Extracts nucleotides downstream of coding sequences for multiple genomes, 
    create <genome>_downstream.fna files in the same directory for each genome.
    Then, classifies/names them relative to gene clusters identified by coding sequence,  
    i.e. after build_cds_pangenome(). See build_proximal_pangenome() for parameters.
    '''
    return build_proximal_pangenome(genome_data, allele_names, output_dir, limits, side='down',
                name=name, include_fragments=include_fragments, fastasort_path=fastasort_path)

    
def build_proximal_pangenome(genome_data, allele_names, output_dir, limits, side,
                             name='Test', include_fragments=False, fastasort_path=None):
    '''
    Extracts nucleotides proximal to coding sequences for multiple genomes, 
    create genome-specific proximal sequence fna files in the same directory for each genome.
    Then, classifies/names them relative to gene clusters identified by coding sequence,  
    i.e. after build_cds_pangenome(). See extract_proximal_sequences() and
    consolidate_proximal() for more details.
    
    Parameters
    ----------
    genome_data : list
        List of 2-tuples (genome_gff, genome_fna) for use by extract_proximal_sequences()
    allele_names : str
        Path to allele names file, should be named <name>_allele_names.tsv
    output_dir : str
        Path to directory to generate summary outputs.
    limits : 2-tuple
        Length of proximal region to extract, formatted (-X,Y). For upstream, extracts X 
        upstream bases (up to but excluding first base of start codon) and first Y coding 
        bases (including first base of start codon). For downstream, extracts the last X
        coding bases and Y downstream bases. In both cases, the total length is X+Y bases.
    side : str
        'up' for upstream, 'down' for downstream
    name : str
        Short header to prepend output summary files, recommendated to be same as what
        was used in the build_cds_pangenome() (default 'Test')
    include_fragments : bool
        If true, include proximal sequences that are not fully available 
        due to contig boundaries (default False)
    fastasort_path : str
        Path to Exonerate's fastasort binary, optionally for sorting
        final FNA files (default None)
        
    Returns
    -------
    df_proximal : pd.DataFrame
        Binary proximal x genome table
    '''
    
    ''' Load header-allele name mapping '''
    print 'Loading header-allele mapping...'
    feature_to_allele = __load_feature_to_allele__(allele_names)
    ftype = {'up':'upstream', 'down':'downstream'}[side]
        
    ''' Generate proximal sequences '''
    print 'Extracting', ftype, 'sequences...'
    genome_proximals = []
    for i, gff_fna in enumerate(genome_data):
        ''' Prepare output path '''
        genome_gff, genome_fna = gff_fna
        genome = genome_gff.split('/')[-1][:-4] # trim off path and .gff
        genome_dir = '/'.join(genome_gff.split('/')[:-1]) + '/' if '/' in genome_gff else ''
        genome_prox_dir = genome_dir + 'derived/'
        if not os.path.exists(genome_prox_dir):
            os.mkdir(genome_prox_dir)
        genome_prox = genome_prox_dir + genome + '_' + ftype + '.fna'
            
        ''' Extract proximal sequences '''
        print i+1, genome
        genome_proximals.append(genome_prox)
        extract_proximal_sequences(genome_gff, genome_fna, genome_prox, 
                                   limits=limits, side=side,
                                   feature_to_allele=feature_to_allele,
                                   include_fragments=include_fragments)
        
    ''' Consolidate non-redundant proximal sequences per gene '''
    print 'Identifying non-redundant', ftype, 'sequences per gene...'
    nr_prox_out = output_dir + '/' + name + '_nr_' + ftype + '.fna'
    nr_prox_out = nr_prox_out.replace('//','/')
    df_proximal = consolidate_proximal(genome_proximals, nr_prox_out, feature_to_allele, side)
    
    ''' Optionally sort non-redundant proximal sequences file '''
    if fastasort_path:
        print 'Sorting sequences by header...'
        args = ['./' + fastasort_path, nr_prox_out]
        with open(nr_prox_out + '.tmp', 'w+') as f_sort:
            sp.call(args, stdout=f_sort)
        os.rename(nr_prox_out + '.tmp', nr_prox_out)
        
    ''' Save proximal x genome table '''
    prox_table_out = output_dir + '/' + name + '_strain_by_' + ftype
    prox_table_out = prox_table_out.replace('//','/')
    prox_table_pickle = prox_table_out + '.pickle.gz'
    prox_table_csv = prox_table_out + '.csv.gz'
    print 'Saving', prox_table_pickle, '...'
    df_proximal.to_pickle(prox_table_pickle)
    print 'Saving', prox_table_csv, '...'
    df_proximal.to_csv(prox_table_csv)
    return df_proximal

    
def consolidate_proximal(genome_proximals, nr_proximal_out, feature_to_allele, side):
    ''' 
    Consolidates proximal sequences to a non-redudnant set with respect to each
    gene described by feature_to_allele (maps_features to <name>_C#A#), then
    creates a proximal x genome binary table. For use with fixed-length 3' or 5'UTRs.
    Upstream features are <name>_C#U#, downstream features are <name>_C#D#.
    
    Parameters
    ----------
    genome_proximals : list
        List of paths to proximal sequences FNA to combine
    nr_proximal_out : str
        Path to output non-redundant proximal sequences as FNA
    feature_to_allele : dict
        Dictionary mapping headers to <name>_C#A# alleles
    side : str
        'up' for upstream, 'down' for downstream
    
    Returns
    -------
    df_proximal : pd.DataFrame
         Binary proximal x genome table
    '''
    
    ftype_abb = {'up':'U', 'down':'D'}[side]
    ftype = {'up':'upstream', 'down':'downstream'}[side]
    gene_to_unique_proximal = {} # maps gene:prox_seq:prox_seq_id (int)
    genome_to_proximal = {} # maps genome:proximal_name:1 if present (<name>_C#U# or <name>_C#D#)
    unique_proximal_ids = set() # record non-redundant list of proximal sequence IDs <name>_C#U# or <name>_C#D#
    genome_order = [] # sorted list of genomes inferred from proximal sequence file names
    
    with open(nr_proximal_out, 'w+') as f_nr_prox:
        for genome_proximal in sorted(genome_proximals):
            ''' Infer genome name from genome filename '''
            genome = genome_proximal.split('/')[-1] # trim off full path
            genome = genome.split('_' + ftype)[0] # remove .fna footer
            genome_to_proximal[genome] = {}
            genome_order.append(genome)
            
            ''' Process genome's proximal record '''
            with open(genome_proximal, 'r') as f_prox: # reading current proximal seq file
                header = ''; prox_seq = ''; new_sequence = False
                for line in f_prox.readlines(): # slight speed up reading whole file at once, should only be few MBs
                    if line[0] == '>': # header line
                        if len(prox_seq) > 0:
                            ''' Process header-seq to non-redundant <name>_C#<U/D># proximal allele '''
                            feature = header.split('_' + ftype + '(')[0] # trim off "_<up/down>stream" footer
                            allele = feature_to_allele[feature] # get <name>_C#A# allele
                            gene = __get_gene_from_allele__(allele) # gene <name>_C# gene
                            if not gene in gene_to_unique_proximal:
                                gene_to_unique_proximal[gene] = {}
                            if not prox_seq in gene_to_unique_proximal[gene]:
                                gene_to_unique_proximal[gene][prox_seq] = len(gene_to_unique_proximal[gene])
                                prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])
                                unique_proximal_ids.add(prox_id)
                                new_sequence = True
                            prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])
                            genome_to_proximal[genome][prox_id] = 1
                            
                            ''' Write renamed sequence to running file '''
                            if new_sequence:
                                f_nr_prox.write('>' + prox_id + '\n')
                                f_nr_prox.write(prox_seq + '\n')
                                new_sequence = False

                        header = line[1:].strip(); prox_seq = ''
                    else: # sequence line
                        prox_seq += line.strip()
            
                ''' Process last record'''
                feature = header.split('_' + ftype + '(')[0] # trim off "_<up/down>stream" footer
                allele = feature_to_allele[feature] # get <name>_C#A# allele
                gene = __get_gene_from_allele__(allele) # gene <name>_C# gene
                if not gene in gene_to_unique_proximal:
                    gene_to_unique_proximal[gene] = {}
                if not prox_seq in gene_to_unique_proximal[gene]:
                    gene_to_unique_proximal[gene][prox_seq] = len(gene_to_unique_proximal[gene])
                    prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])
                    unique_proximal_ids.add(prox_id)
                    new_sequence = True
                prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])
                genome_to_proximal[genome][prox_id] = 1

                ''' Write renamed sequence to running file '''
                if new_sequence:
                    f_nr_prox.write('>' + prox_id + '\n')
                    f_nr_prox.write(prox_seq + '\n')
                    new_sequence = False
                    
    ''' Convert nested dict to dict of genome:SparseArrays once all proximal sequences are known '''
    print 'Sparsifying', ftype, 'table...'
    prox_order = sorted(list(unique_proximal_ids))
    del unique_proximal_ids
    prox_indices = {prox_order[i]:i for i in range(len(prox_order))} # map proximal ID to index
    for g,genome in enumerate(genome_order):
        prox_array = np.zeros(shape=len(prox_order), dtype='int64')
        for genome_prox in genome_to_proximal[genome].keys():
            prox_i = prox_indices[genome_prox]
            prox_array[prox_i] = 1
        genome_to_proximal[genome] = pd.SparseArray(prox_array)
        genome_to_proximal[genome].fill_value = np.nan
        
    print 'Constructing DataFrame...'
    df_proximal = pd.DataFrame(data=genome_to_proximal, index=prox_order)
    return df_proximal


def extract_upstream_sequences(genome_gff, genome_fna, upstream_out, limits=(-50,3), 
                               feature_to_allele=None, allele_names=None, include_fragments=False):
    '''
    Extracts nucleotides upstream of coding sequences to file, default 50bp + start codon.
    Refer to extract_proximal_sequences() for parameters.
    '''
    extract_proximal_sequences(genome_gff, genome_fna, proximal_out=upstream_out, 
                               limits=limits, side='up', feature_to_allele=feature_to_allele, 
                               allele_names=allele_names, include_fragments=include_fragments)

    
def extract_downstream_sequences(genome_gff, genome_fna, downstream_out, limits=(-3,50), 
                                 feature_to_allele=None, allele_names=None, include_fragments=False):
    '''
    Extracts nucleotides downstream of coding sequences to file, default stop codon + 50 bp.
    Refer to extract_proximal_sequences() for parameters.
    '''
    extract_proximal_sequences(genome_gff, genome_fna, proximal_out=downstream_out, 
                               limits=limits, side='down', feature_to_allele=feature_to_allele, 
                               allele_names=allele_names, include_fragments=include_fragments)
    
                
def extract_proximal_sequences(genome_gff, genome_fna, proximal_out, limits, side,
                               feature_to_allele=None, allele_names=None, include_fragments=False):
    '''
    Extracts nucleotides upstream or downstream of coding sequences. 
    Interprets GFFs as formatted by PATRIC:
        1) Assumes contigs are labeled "accn|<contig>". 
        2) Assumes protein features have ".peg." in the ID
        3) Assumes ID = fig|<genome>.peg.#
    Output features are named "<feature header>_<up/down>stream(<limit1>,<limit2>)". 
        
    Parameters
    ----------
    genome_gff : str
        Path to genome GFF file with CDS coordinates
    genome_fna : str
        Path to genome FNA file with contig nucleotides
    proximal_out : str
        Path to output upstream/downstream sequences FNA files
    limits : 2-tuple
        Length of proximal region to extract, formatted (-X,Y). For upstream, extracts X 
        upstream bases (up to but excluding first base of start codon) and first Y coding 
        bases (including first base of start codon). For downstream, extracts the last X
        coding bases and Y downstream bases. In both cases, the total length is X+Y bases.
    side : str
        'up' if upstream, 'down' if downstream
    feature_to_allele : dict, str
        Dictionary mapping original feature headers to <name>_C#A# short names,
        alternatively, the allele_names file can be provided (default None)
    allele_names : str
        Path to allele names file if feature_to_allele is not provided,
        should be named <name>_allele_names.tsv. If neither are provided,
        simply processes all features present in the GFF (default None)
    include_fragments : bool
        If true, include upstream sequences that are not fully available 
        due to contig boundaries (default False)
    '''
    
    ''' Load contig sequences '''
    contigs = load_sequences_from_fasta(genome_fna, header_fxn=lambda x: x.split()[0])
            
    ''' Load header-allele name mapping '''
    if feature_to_allele: # dictionary provided directly
        feat_to_allele = feature_to_allele
    elif allele_names: # allele map file provided
        feat_to_allele = __load_feature_to_allele__(allele_names)
    else: # no allele mapping, process everything
        feat_to_allele = None
                    
    ''' Parse GFF file for CDS coordinates '''
    feature_footer = '_downstream' if side == 'down' else '_upstream'
    feature_footer += str(limits).replace(' ','')
    
    proximal_count = 0
    with open(proximal_out, 'w+') as f_prox:
        with open(genome_gff, 'r') as f_gff:
            for line in f_gff:
                line = line.strip()
                if len(line) > 0 and line[0] != '#':
                    contig, src, feat_type, start, stop, score, strand, phase, attr_raw = line.split('\t')
                    contig = contig.split('|')[-1] # accn|<contig> to just <contig>
                    start = int(start); stop = int(stop)
                    attrs = {} # key:value
                    for entry in attr_raw.split(';'):
                        k,v = entry.split('='); attrs[k] = v
                    gffid = attrs['ID']

                    ''' Verify allele has been mapped, and contig has been identified '''
                    if contig in contigs: 
                        if gffid in feat_to_allele or feat_to_allele is None:
                            contig_seq = contigs[contig]
                            ''' Identify proximal sequence '''
                            if side == 'down': # downstream sequence
                                if strand == '+': # positive strand
                                    down_start = stop + limits[0]
                                    down_end = stop + limits[1]
                                    proximal = contig_seq[down_start:down_end].strip()
                                else: # negative strand
                                    down_start = start - limits[1] - 1
                                    down_end = start - limits[0] - 1
                                    proximal = contig_seq[down_start:down_end].strip()
                                    proximal = reverse_complement(proximal)
                            else: # default to upstream sequence
                                if strand == '+': # positive strand
                                    ups_start = start + limits[0] - 1
                                    ups_end = start + limits[1] - 1
                                    proximal = contig_seq[ups_start:ups_end].strip()
                                else: # negative strand
                                    ups_start = stop - limits[1]
                                    ups_end = stop - limits[0] 
                                    proximal = contig_seq[ups_start:ups_end].strip()
                                    proximal = reverse_complement(proximal)
                                
                            ''' Save upstream sequence '''
                            if len(proximal) == -limits[0] + limits[1] or include_fragments:
                                feat_name = gffid + feature_footer
                                f_prox.write('>' + feat_name + '\n')
                                f_prox.write(proximal + '\n')
                                proximal_count += 1
    
    ftype = 'downstream' if side == 'down' else 'upstream'
    print 'Loaded', ftype, 'sequences:', proximal_count

    
def extract_noncoding(genome_gff, genome_fna, noncoding_out, 
                      allowed_features=['transcript', 'tRNA', 'rRNA', 'misc_binding']):
    '''
    Extracts nucleotides for non-coding sequences. 
    Interprets GFFs as formatted by PATRIC:
        1) Assumes contigs are labeled "accn|<contig>". 
        2) Assumes protein features have ".peg." in the ID
        3) Assumes ID = fig|<genome>.peg.#
        
    Parameters
    ----------
    genome_gff : str
        Path to genome GFF file with CDS coordinates
    genome_fna : str
        Path to genome FNA file with contig nucleotides
    noncoding_out : str
        Path to output transcript sequences FNA files
    allowed_features : list
        List of GFF feature types to extract. Default excludes 
        features labeled "CDS" or "repeat_region" 
        (default ['transcript', 'tRNA', 'rRNA', 'misc_binding'])
    '''
    contigs = load_sequences_from_fasta(genome_fna, header_fxn=lambda x:x.split()[0])
    with open(noncoding_out, 'w+') as f_noncoding:
        with open(genome_gff, 'r') as f_gff:
            for line in f_gff:
                ''' Check for non-comment and non-empty line '''
                if not line[0] == '#' and not len(line.strip()) == 0: 
                    contig, src, feature_type, start, stop, \
                        score, strand, phase, meta = line.split('\t')
                    contig = contig[5:] # trim off "accn|" header
                    start = int(start)
                    stop = int(stop)
                    
                    if feature_type in allowed_features: 
                        ''' Get noncoding feature sequence and ID '''
                        contig_seq = contigs[contig]
                        feature_seq = contig_seq[start-1:stop]
                        if strand == '-': # negative strand
                            feature_seq = reverse_complement(feature_seq)
                        meta_key_vals = map(lambda x: x.split('='), meta.split(';'))
                        metadata = {x[0]:x[1] for x in meta_key_vals}
                        feature_id = metadata['ID']
                        
                        ''' Save to output file '''
                        feature_seq = '\n'.join(feature_seq[i:i+70] for i in range(0, len(feature_seq), 70))
                        f_noncoding.write('>' + feature_id + '\n')
                        f_noncoding.write(feature_seq + '\n') 

    
def validate_gene_table(df_genes, df_alleles):
    '''
    Verifies that the gene x genome table is consistent with the
    corresponding allele x genome table. Optimized to run column-by-column
    rather than gene-by-gene for sparse tables.
    
    Parameters
    ----------
    df_genes : pd.DataFrame or str
        Either the gene x genome table, or path to the table
    df_alleles : pd.DataFrame or str
        Either the allele x genome table, or path to the table
    '''
    dfg = load_feature_table(df_genes)
    dfa = load_feature_table(df_alleles)
    print 'Validating gene clusters...'
    num_inconsistencies = 0
    for g,genome in enumerate(df_genes.columns):
        print g+1, 'Testing', genome
        genes = set(dfg[genome].dropna().index)
        alleles = dfa[genome].dropna().index
        allele_genes = set(alleles.map(__get_gene_from_allele__))
        if genes != allele_genes:
            inconsistencies = genes.symmetric_difference(allele_genes)
            print '\tInconsistent:', inconsistencies
            num_inconsistencies += len(inconsistencies)
    
    print 'Total Inconsistencies:', num_inconsistencies 


def validate_gene_table_dense(df_genes, df_alleles):
    '''
    Verifies that the gene x genome table is consistent with the
    corresponding allele x genome table. Original approach for
    when df_genes and df_alleles are dense DataFrames.
    
    Parameters
    ----------
    df_genes : pd.DataFrame or str
        Either the gene x genome table, or path to the table
    df_alleles : pd.DataFrame or str
        Either the allele x genome table, or path to the table
    '''
    dfg = load_feature_table(df_genes)
    dfa = load_feature_table(df_alleles)
    print 'Validating gene clusters...'
    
    current_cluster = None; allele_data = []; 
    clusters_tested = 0; inconsistencies = 0
    for allele_row in dfa.fillna(0).itertuples(name=None):
        cluster = __get_gene_from_allele__(allele_row[0])
        if current_cluster is None: # initializing
            current_cluster = cluster
        elif current_cluster != cluster: # end of gene cluster
            alleles_all = np.array(allele_data)
            has_gene = alleles_all.sum(axis=0) > 0
            is_consistent = np.array_equal(has_gene, dfg.loc[current_cluster,:].fillna(0).values)
            clusters_tested += 1
            if not is_consistent:
                print 'Inconsistent', cluster
                print has_gene
                print dfg.loc[current_cluster,:].fillna(0).values
                inconsistencies += 1
            if clusters_tested % 1000 == 0:
                print '\tTested', clusters_tested, 'clusters'
            allele_data = []
            current_cluster = cluster
        allele_data.append(np.array(allele_row[1:]))
    
    ''' Process final line '''
    alleles_all = np.array(allele_data) 
    has_gene = alleles_all.sum(axis=0) > 0
    is_consistent = np.array_equal(has_gene, dfg.loc[current_cluster,:].fillna(0).values)
    if not is_consistent:
        print 'Inconsistent', cluster
        print has_gene
        print dfg.loc[current_cluster,:].fillna(0).values
        inconsistencies += 1
    print 'Inconsistencies:', inconsistencies


def validate_allele_table(df_alleles, genome_faa_paths, alleles_faa):
    '''
    Verifies that the allele x genome table is consistent with the original FAA files.
    
    Parameters
    ----------
    df_alleles : pd.DataFrame or str
        Either the allele x genome table, or path to the table as CSV or CSV.GZ
    genome_faa_paths : list
        Paths to genome FAA files originally combined and clustered
    alleles_faa : str
        Path to non-redundant sequences corresponding to df_alleles
    '''
    dfa = load_feature_table(df_alleles)
    inconsistencies = 0
    
    ''' Pre-load hashes for non-redundant protein sequences '''
    print 'Loading non-redundant sequences...'
    seqhash_to_allele = {}
    with open(alleles_faa, 'r') as f_faa:
        header = ''; seq_blocks = []
        for line in f_faa:
            if line[0] == '>': # new sequence encountered
                if len(seq_blocks) > 0:
                    seqhash = __hash_sequence__(''.join(seq_blocks))
                    seqhash_to_allele[seqhash] = header
                header = line[1:].strip()
                seq_blocks = []
            else: # sequence encountered
                seq_blocks.append(line.strip())
        # process last record
        seqhash = __hash_sequence__(''.join(seq_blocks))
        seqhash_to_allele[seqhash] = header
                        
    ''' Validate individual genomes against table '''
    allele_counts = dfa.sum() # genome x total alleles
    for i, genome_faa in enumerate(sorted(genome_faa_paths)):
        print 'Validating genome', i+1, ':', genome_faa, 
        
        ''' Load all alleles present in the genome '''
        genome_alleles = set()
        with open(genome_faa, 'r') as f_faa:
            allele = ''; seq_blocks = []
            for line in f_faa:
                if line[0] == '>': # new sequence encountered
                    seq = ''.join(seq_blocks)
                    if len(seq) > 0:
                        seqhash = __hash_sequence__(seq)
                        allele = seqhash_to_allele[seqhash]
                        genome_alleles.add(allele)
                    seq_blocks = []
                else: # sequence encountered
                    seq_blocks.append(line.strip())
            # process last record
            seq = ''.join(seq_blocks)
            if len(seq) > 0:
                seqhash = __hash_sequence__(seq)
                allele = seqhash_to_allele[seqhash]
                genome_alleles.add(allele)
            
        ''' Check that identified alleles are consistent with the table '''
        genome = genome_faa.split('/')[-1][:-4]
        df_ga = dfa.loc[:,genome]
        table_alleles = set(df_ga.index[pd.notnull(df_ga)])
        test = table_alleles == genome_alleles
        inconsistencies += (1 - int(test))
        print test, len(table_alleles), len(genome_alleles)
    print 'Inconsistencies:', inconsistencies
        

def validate_upstream_table(df_upstream, genome_fna_paths, nr_upstream_fna, limits=(-50,3)):
    '''
    Does a partial validation of the upstream x genome table by checking that
    the recorded upstream sequences are present in the corresponding genome, 
    and counts start codons observed. DOES NOT check the exact location of the
    upstream sequences.
    
    Parameters
    ----------
    df_upstream : pd.DataFrame or str
        Either the upstream x genome table, or path to the table as CSV or CSV.GZ
    genome_fna_paths : list
        Paths to FNAs for each genome's contigs
    nr_upstream_fna : str
        Path to FNA with non-redundant upstream sequences per gene
    limits : 2-tuple
        Upstream sequence limits specified when extracting upstream sequences (default (-50,3))
    '''
    dfu = load_feature_table(df_upstream)
    
    ''' Reverse complement function for checking both strands '''
    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 
                  'W': 'W', 'S': 'S', 'R': 'Y', 'Y': 'R', 
                  'M': 'K', 'K': 'M', 'N': 'N'}
    for bp in complement.keys():
        complement[bp.lower()] = complement[bp].lower()
    reverse_complement = lambda s: (''.join([complement[base] for base in list(s)]))[::-1]
    
    ''' Load non-redundant upstream sequences '''
    print 'Loading upstream sequences...'
    nr_upstream = load_sequences_from_fasta(nr_upstream_fna)
            
    ''' Verify present of each upstream sequence within each genome '''
    window = limits[1] - limits[0]
    for g, genome_fna in enumerate(genome_fna_paths):
        genome_contigs = load_sequences_from_fasta(genome_fna)
        genome = genome_fna.split('/')[-1][:-4]
        print g+1, 'Evaluating', genome, genome_fna
        df_gups = dfu.loc[:,genome]
        table_ups = df_gups.index[pd.notnull(df_gups)] # upstream sequences as defined by the table
        table_ups_seqs = {nr_upstream[x]:x for x in table_ups} # maps sequences to names
        
        ''' Scan all contigs for detected upstream sequences '''
        for contig in genome_contigs.values():
            for i in range(len(contig)):
                segment = contig[i:i+window]
                if segment in table_ups_seqs: 
                    table_ups_seqs.pop(segment)
            rc_contig = reverse_complement(contig)
            for i in range(len(rc_contig)):
                segment = rc_contig[i:i+window]
                if segment in table_ups_seqs: 
                    table_ups_seqs.pop(segment)
                    
        ''' Report undetected upstream sequences '''
        for ups in table_ups_seqs:
            print '\tMissing', table_ups_seqs[ups], 'from', genome
        
    ''' Count start codons among non-redundant upstream sequences '''
    if limits[1] >= 3:
        print 'Computing start codon distribution...'
        if limits[1] == 3:
            get_start = lambda x: x[-3:]
        else:
            get_start = lambda x: x[-limits[1]:-limits[1]+3]
        start_codons = map(get_start, nr_upstream.values())
        print collections.Counter(start_codons)
    

def load_sequences_from_fasta(fasta, header_fxn=None, seq_fxn=None):
    ''' Loads sequences from a FAA or FNA file into a dict
        mapping headers to sequences. Can optionally apply a 
        function to all header (header_fxn) or to all
        sequences (seq_fxn). Drops the ">" from all headers,
        and removes line breaks from sequences. '''
    header_to_seq = {}
    with open(fasta, 'r') as f:
        header = ''; seq = ''
        for line in f:
            if line[0] == '>': # header line
                if len(header) > 0 and len(seq) > 0:
                    seq = seq_fxn(seq) if seq_fxn else seq
                    header_to_seq[header] = seq
                header = line.strip()[1:]
                header = header_fxn(header) if header_fxn else header
                seq = ''
            else: # sequence line
                seq += line.strip()
        if len(header) > 0 and len(seq) > 0:
            seq = seq_fxn(seq) if seq_fxn else seq
            header_to_seq[header] = seq
    return header_to_seq

def load_feature_table(feature_table):
    ''' 
    Loads DataFrames from CSV, CSV.GZ, PICKLE, or PICKLE.GZ.
    Uses index_col=0 for CSVs. Returns feature_table if provided 
    with anything other than a string.
    '''
    if type(feature_table) == str: # path provided
        if feature_table[-4:].lower() == '.csv' or feature_table[-7:].lower() == '.csv.gz':
            return pd.read_csv(feature_table, index_col=0)
        elif feature_table[-7:].lower() == '.pickle' or feature_table[-10:].lower() == '.pickle.gz':
            return pd.read_pickle(feature_table)
        else:
            return feature_table
    else: # non-string input
        return feature_table
    
    
def reverse_complement(seq):
    ''' Returns the reverse complement of a DNA sequence.
        Supports lower/uppercase and ambiguous bases'''
    return ''.join([DNA_COMPLEMENT[base] for base in list(seq)])[::-1]


def __create_sparse_data_frame__(sparse_array, index, columns):
    ''' 
    Creates a SparseDataFrame from a scipy.sparse matrix by initializing 
    individual columns as SparseSeries, then concatenating them. 
    Sometimes faster than native SparseDataFrame initialization? Known issues: 
    
    - Direct initialization is slow in v0.24.2, see https://github.com/pandas-dev/pandas/issues/16773
    - Empty SparseDataFrame initialization time scales quadratically with number of columns
    - Initializing as dense and converting sparse uses more memory than directly making sparse
    '''
    n_row, n_col = sparse_array.shape
    X = sparse_array.T if n_row < n_col else sparse_array # make n_col < n_row
    sparse_cols = []
    for i in range(X.shape[1]): # create columns individually
        sparse_col = pd.SparseSeries.from_coo(scipy.sparse.coo_matrix(X[:,i]), dense_index=True)
        sparse_cols.append(sparse_col)
    df = pd.concat(sparse_cols, axis=1)
    df = df.T if n_row < n_col else df # transpose back if n_col > n_row originally
    df.index = pd.Index(index)
    df.columns = pd.Index(columns)
    return df


def __load_feature_to_allele__(allele_names):
    ''' Loads feature-to-allele mapping from file, usually <name>_allele_names.tsv. '''
    map_feature_to_gffid = lambda x: '|'.join(x.split('|')[:2])
    feat_to_allele = {}
    with open(allele_names, 'r') as f_all:
        for line in f_all:
            data = line.strip().split('\t')
            allele = data[0]; synonyms = data[1:]
            for synonym in synonyms:
                gff_synonym = map_feature_to_gffid(synonym)
                feat_to_allele[gff_synonym] = allele
    return feat_to_allele
                          
def __get_gene_from_allele__(allele):
    ''' Converts <name>_C#A# allele to <name>_C# gene '''
    return 'A'.join(allele.split('A')[:-1])

def __get_header_from_fasta_line__(line):
    ''' Extracts a short header from a full header line in a fasta'''
    return line.split()[0][1:].strip()

def __hash_sequence__(seq):
    ''' Hashes arbitary length strings/sequences '''
    return hashlib.sha256(seq).hexdigest()
            
def __stream_stdout__(command):
    ''' Hopefully Jupyter-safe method for streaming process stdout '''
    process = sp.Popen(command, stdout=sp.PIPE, shell=True)
    while True:
        line = process.stdout.readline()
        if not line:
            break
        yield line.rstrip()
    